{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV 파일을 불러올 때 Schema를 추정할 수 있다. 애당초 Header가 잘 돼 있으면 헤더만으로도 충분한 듯.\n",
    "flightData2015=spark\\\n",
    ".read\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"/Users/talysa/Documents/Study/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#잘 가져왔나 찍어보자.\n",
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count 값을 기준으로 정렬한다.\n",
    "flightData2015.sort(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파티션 수를 5개로 줄여보자. 어차피 콩알만한 데이터라 금방 끝난다.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#102 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#102 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#100,ORIGIN_COUNTRY_NAME#101,count#102] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/talysa/Documents/Study/Spark-The-Definitive-Guide/data/flight-data/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "#정렬이 어떤 식으로 수행되는지 확인해보자. 파일을 이러이러한 조건 걸고 스캔-설정된 파티션 수대로 일하기로 했다-정렬한다\n",
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정렬된 데이터를 찍어보자. count가 낮은 순서로 나온다.\n",
    "flightData2015.sort(\"count\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#읽어온 데이터로 테이블을 만들자!\n",
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이제 sql로 질의를 날릴 수 있다. \n",
    "sqlWay=spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#근데 사실 저렇게 테이블 안 만들어도 이런건 그대로 할 수 있다. \n",
    "dataFrameWay=flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#100], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#100, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#100], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#100] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/talysa/Documents/Study/Spark-The-Definitive-Guide/data/flight-data/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "#테이블에서 데이터를 어떻게 가져오나 본다.\n",
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#100], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#100, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#100], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#100] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/talysa/Documents/Study/Spark-The-Definitive-Guide/data/flight-data/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "#테이블을 만들지 않은 경우에는 데이터를 어떻게 가져오나 본다. 근데 똑같음. 그러니까 성능상으로도 차이는 없나보다.\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#제일 큰 값을 가져오는 max()다. sql 사용할 때는 그냥 이렇게 하면 된다.\n",
    "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[max(count#102)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_max(count#102)])\n",
      "      +- *(1) FileScan csv [count#102] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/talysa/Documents/Study/Spark-The-Definitive-Guide/data/flight-data/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<count:int>\n"
     ]
    }
   ],
   "source": [
    "#어떻게 돌아가는지 실행 계획을 까본다.\n",
    "spark.sql(\"SELECT max(count) from flight_data_2015\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sql 안 쓰면 이렇게 import 해줘야 한다.\n",
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#마찬가지로 max를 날려보자. 결과는 당연히 같다.\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[max(count#102)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_max(count#102)])\n",
      "      +- *(1) FileScan csv [count#102] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/talysa/Documents/Study/Spark-The-Definitive-Guide/data/flight-data/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<count:int>\n"
     ]
    }
   ],
   "source": [
    "#얘는 동작 방식이 다른가 까보자. 똑같다!\n",
    "flightData2015.select(max(\"count\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 나라들이 목적지로 쓰인 횟수를 집계한다. sql로. destination_total 기준으로 내림차순 정렬함.\n",
    "maxSql=spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[aggOrder#284L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#100,destination_total#282L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#100], functions=[sum(cast(count#102 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#100, 5)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#100], functions=[partial_sum(cast(count#102 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#100,count#102] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/talysa/Documents/Study/Spark-The-Definitive-Guide/data/flight-data/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "#출력한다. 실행계획도 까보자.\n",
    "maxSql.show()\n",
    "maxSql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이번에는 sql 안 쓰고 같은 데이터를 요청해봅니다. 일단 내림차순은 import 해야함.\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#427L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#100,destination_total#427L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#100], functions=[sum(cast(count#102 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#100, 5)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#100], functions=[partial_sum(cast(count#102 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#100,count#102] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/talysa/Documents/Study/Spark-The-Definitive-Guide/data/flight-data/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "#sql 질의문보다 복잡해보인다! 귀찮지만 실행한다. 같은 결과가 나온다. \n",
    "#실행계획을 읽어보면 파일 스캔-파티션-destination_total로 내림차순 정렬-limit 5개만 꺼내다 출력\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".show()\n",
    "\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
